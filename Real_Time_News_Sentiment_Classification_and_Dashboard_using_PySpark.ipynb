{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM46Ylll3qitQLPsOeYdMEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shahanas2003/Capstone_project/blob/main/Real_Time_News_Sentiment_Classification_and_Dashboard_using_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, json, time, uuid, os\n",
        "from datetime import datetime\n",
        "\n",
        "API_KEY = \"pub_6a78f4ac6f74492996375d87da57ae7f\"\n",
        "NEWS_API_URL = f\"https://newsdata.io/api/1/news?apikey={API_KEY}&language=en&country=us\"\n",
        "SAVE_DIR = \"/content/news_stream\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "def fetch_and_save_news():\n",
        "    try:\n",
        "        resp = requests.get(NEWS_API_URL)\n",
        "        news = resp.json().get(\"results\", [])\n",
        "        file_path = os.path.join(SAVE_DIR, f\"{uuid.uuid4()}.json\")\n",
        "        with open(file_path, \"w\") as f:\n",
        "            for article in news:\n",
        "                json.dump({\n",
        "                    \"title\": article.get(\"title\", \"\"),\n",
        "                    \"pubDate\": article.get(\"pubDate\", \"\")\n",
        "                }, f)\n",
        "                f.write(\"\\n\")\n",
        "        print(f\"Saved {len(news)} articles at {datetime.now()}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "# Fetch once (for testing)\n",
        "fetch_and_save_news()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFrq-t-ADDRU",
        "outputId": "29751f01-b2b3-4457-ac0a-6fc7e786bcce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 10 articles at 2025-09-28 06:40:57.356807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = SparkSession.builder.appName(\"NewsSentiment\").getOrCreate()\n",
        "\n",
        "# Sample labeled training data\n",
        "train_data = [\n",
        "    (\"Stock market hits all-time high\", \"positive\"),\n",
        "    (\"War breaks out in eastern region\", \"negative\"),\n",
        "    (\"Company reports strong quarterly earnings\", \"positive\"),\n",
        "    (\"Earthquake leaves thousands homeless\", \"negative\"),\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"text\", StringType(), True),\n",
        "    StructField(\"label\", StringType(), True),\n",
        "])\n",
        "\n",
        "df_train = spark.createDataFrame(train_data, schema)\n",
        "\n",
        "# ML pipeline\n",
        "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"labelIndex\")\n",
        "\n",
        "pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, tf, idf, lr])\n",
        "model = pipeline.fit(df_train)\n",
        "\n",
        "# Save trained model\n",
        "model.save(\"/content/news_sentiment_model\")\n"
      ],
      "metadata": {
        "id": "kOmTspIWDFwp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "# Load model\n",
        "model = PipelineModel.load(\"/content/news_sentiment_model\")\n",
        "\n",
        "# Schema for streaming data\n",
        "stream_schema = StructType([\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"pubDate\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read stream\n",
        "stream_df = spark.readStream.schema(stream_schema).json(\"/content/news_stream\")\n",
        "\n",
        "# Rename for consistency\n",
        "stream_df = stream_df.withColumnRenamed(\"title\", \"text\")\n",
        "\n",
        "# Apply model\n",
        "predictions = model.transform(stream_df)\n",
        "\n",
        "# Select only useful columns\n",
        "result = predictions.select(\"text\", \"prediction\", \"pubDate\")\n",
        "\n",
        "# Write predictions to folder\n",
        "query = result.writeStream.outputMode(\"append\") \\\n",
        "    .format(\"json\") \\\n",
        "    .option(\"path\", \"/content/predictions\") \\\n",
        "    .option(\"checkpointLocation\", \"/content/checkpoint\") \\\n",
        "    .start()\n"
      ],
      "metadata": {
        "id": "U_nQV5HyDjCB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import glob, json\n",
        "\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"ðŸ“° Real-Time News Sentiment Dashboard\")\n",
        "\n",
        "files = glob.glob(\"/content/predictions/*.json\")\n",
        "data = []\n",
        "\n",
        "for file in files:\n",
        "    with open(file) as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "if not data:\n",
        "    st.warning(\"Waiting for predictions...\")\n",
        "else:\n",
        "    df = pd.DataFrame(data)\n",
        "    df['sentiment'] = df['prediction'].apply(lambda x: 'Positive' if x == 1.0 else 'Negative')\n",
        "\n",
        "    st.write(\"### Latest Predictions\")\n",
        "    st.table(df[['text', 'pubDate', 'sentiment']].tail(10))\n",
        "\n",
        "    st.write(\"### Sentiment Distribution\")\n",
        "    st.bar_chart(df['sentiment'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4rFjt4iExoG",
        "outputId": "f6b9c845-480c-470a-a079-5edb70198454"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}